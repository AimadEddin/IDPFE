{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qKTRB-wekF0"
      },
      "outputs": [],
      "source": [
        "# Mettre à jour la liste des paquets disponibles\n",
        "!apt-get update\n",
        "\n",
        "# Installer Chromium et Chromedriver\n",
        "!apt-get install -y chromium-chromedriver\n",
        "\n",
        "# Installer les bibliothèques Python nécessaires\n",
        "!pip install selenium beautifulsoup4 pandas\n",
        "\n",
        "# Copier Chromedriver dans le répertoire approprié\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "# Importer les bibliothèques nécessaires\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Configurer le webdriver (assurez-vous d'avoir le driver correct pour votre navigateur)\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# URL du site web\n",
        "# pour avoir les startups à succès :\n",
        "#url = \"https://www.failory.com/interviews?type-of-interview=Successful%20startup\"\n",
        "# pour avoir les startups échouées :\n",
        "url = \"https://www.failory.com/interviews?type-of-interview=Failed%20startup\"\n",
        "driver.get(url)\n",
        "\n",
        "# Faire défiler jusqu'en bas pour charger toutes les données\n",
        "SCROLL_PAUSE_TIME = 2\n",
        "\n",
        "# Obtenir la hauteur de défilement\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Faire défiler vers le bas de la page\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "    # Attendre que la page se charge\n",
        "    time.sleep(SCROLL_PAUSE_TIME)\n",
        "\n",
        "    # Calculer la nouvelle hauteur de défilement et comparer avec l'ancienne hauteur de défilement\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "\n",
        "# Analyser la source de la page avec BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Trouver toutes les listes de startups\n",
        "listings = soup.select('div.w-dyn-items > div')\n",
        "\n",
        "# Liste pour stocker toutes les données extraites\n",
        "data = []\n",
        "\n",
        "for listing in listings:\n",
        "    problem = listing.select_one('a > div.card-title').get_text(strip=True)\n",
        "    sector = listing.select_one('div.interviews-filters-div-block.w-clearfix > div:nth-child(3)').get_text(strip=True)\n",
        "    country = listing.select_one('div.interviews-filters-div-block.w-clearfix > div:nth-child(4)').get_text(strip=True)\n",
        "    reason = listing.select_one('div.interviews-filters-div-block.w-clearfix > div:nth-child(5)').get_text(strip=True)\n",
        "    profit = listing.select_one('div.interviews-filters-div-block.w-clearfix > div:nth-child(6)').get_text(strip=True)\n",
        "    founder = listing.select_one('div.interviews-card-data-div-block > div > div.interviews-card-name').get_text(strip=True)\n",
        "    date = listing.select_one('div.interviews-card-data-div-block > div > div.card-date').get_text(strip=True)\n",
        "    link = listing.select_one('a')['href']\n",
        "    startup_name = link.split('/')[-1]\n",
        "\n",
        "    # Ouvrir la page de détails\n",
        "    driver.get(f\"https://www.failory.com{link}\")\n",
        "    details_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    summary = details_soup.select_one('#content > p').get_text(strip=True)\n",
        "\n",
        "    # Stocker les données extraites\n",
        "    data.append({\n",
        "        'Problème': problem,\n",
        "        'Secteur': sector,\n",
        "        'Pays': country,\n",
        "        'Raison': reason,\n",
        "        'Profit': profit,\n",
        "        'Fondateur': founder,\n",
        "        'Date': date,\n",
        "        'Nom de la startup': startup_name,\n",
        "        'Résumé': summary\n",
        "    })\n",
        "\n",
        "    # Retourner à la page principale\n",
        "    driver.back()\n",
        "\n",
        "# Convertir en DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('startup_failed.csv', index=False)\n",
        "\n",
        "# Fermer le driver\n",
        "driver.quit()\n",
        "\n",
        "print(\"Extraction terminée. Les données sont sauvegardées dans startup_failed.csv.\")"
      ]
    }
  ]
}